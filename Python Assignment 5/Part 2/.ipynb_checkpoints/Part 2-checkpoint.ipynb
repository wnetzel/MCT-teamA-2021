{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8d9ae211",
   "metadata": {},
   "source": [
    "# Python Assignment 5 part 2 by Team A "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0566937c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import librosa\n",
    "import pretty_midi\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import sounddevice as sd\n",
    "import soundfile as sf\n",
    "import librosa.display\n",
    "import math\n",
    "import scipy\n",
    "import time\n",
    "from scipy import signal"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd23a1dc",
   "metadata": {},
   "source": [
    "# Mixing a track in Python\n",
    "\n",
    "In this project we have tried to mix a multitrack song using only Python and self-designed FX and tools. We made five tracks of unprocessed audio (Programmed in midi in Cubase 11), and load them into this program to process each of the tracks and then sum together for a final mix. Obviously Python is not a very good option for mixing a song, but we thought it was an interesting challenge and a way to really push all we have learned about signal processing so far this term. \n",
    "\n",
    "The original unprocessed total of the music can be heard in the cell below. \n",
    "In the end of this assignment we display the waveforms in the style of both Audacity and Serato/Traktor. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21c59c26",
   "metadata": {},
   "outputs": [],
   "source": [
    "sr = 48000\n",
    "\n",
    "org_mix, r3 = librosa.load('audio/py5-fullmix.wav', sr=sr)\n",
    "ipd.Audio(org_mix, rate=48000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1482f1",
   "metadata": {},
   "source": [
    "### Loading the tracks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc44d3a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importing audio file\n",
    "\n",
    "s, r = librosa.load('audio/py5-Hihats and toms.wav', sr=sr)\n",
    "s2, r2 = librosa.load('audio/py5-Main drums.wav', sr=sr)\n",
    "s3, r5 = librosa.load('audio/py5-Pluck synth.wav', sr=sr)\n",
    "brass, r3 = librosa.load('audio/py5-Brass stabs.wav', sr=sr)\n",
    "bass, r4 = librosa.load('audio/py5-Bass synth.wav', sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "823f4b96",
   "metadata": {},
   "source": [
    "### Functions\n",
    "\n",
    "Here are the functions needed to run the program. The first cell contains the functions we use to slice, pad and stitch together the audio in the processing part of the program."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa63f09",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to add fadeins and fadeouts to the segments to avoid clicks\n",
    "def fade(segment, fadesize):\n",
    "    fades = np.array([])\n",
    "    fadesize = fadesize\n",
    "    if segment.size <= (fadesize*2):\n",
    "        fadesize = int(segment.size / 10)\n",
    "        midpart = np.ones(segment.size - (int(fadesize*2)))\n",
    "        fadein = np.linspace(0,1,num=fadesize)\n",
    "        fadeout = np.linspace(1,0,num=fadesize)\n",
    "        fades = np.concatenate((fadein, midpart, fadeout))\n",
    "        \n",
    "    else:         \n",
    "        midpart = np.ones(segment.size - (int(fadesize*2)))   \n",
    "        fadein = np.linspace(0,1,num=fadesize)\n",
    "        fadeout = np.linspace(1,0,num=fadesize)\n",
    "        fades = np.concatenate((fadein, midpart, fadeout))\n",
    "        \n",
    "    y = segment * fades\n",
    "    \n",
    "    return y\n",
    "\n",
    "# Function to check if a segment has the correct size, and if not to make sure it is right\n",
    "# by padding or deleting samples. \n",
    "def sizecheck(segment, result):\n",
    "    if segment.size != result.size:\n",
    "        if segment.size > result.size:\n",
    "            diff = segment.size-result.size            \n",
    "            segment = np.delete(segment_final, diff)\n",
    "            \n",
    "        elif result.size > segment.size:\n",
    "            zeros= np.zeros(result.size - segment.size)\n",
    "            \n",
    "            segment = np.concatenate((segment, zeros))\n",
    "    else:\n",
    "        pass\n",
    "    \n",
    "    return segment\n",
    "\n",
    "# Main function to zeropad a segment before we paste it back into the final result\n",
    "def padder(segment, audiototal, current_beat):\n",
    "    y = np.pad(segment, [current_beat, audiototal.size-(current_beat+segment.size)])\n",
    "    return y\n",
    "\n",
    "# Function to overlap segments. Cutting a small part in the beginning\n",
    "# and apply padding equal to the overlap in the end\n",
    "def overlapper(segment, overlap):\n",
    "    x = segment[overlap:segment.size]\n",
    "    y = np.pad(x, [0, overlap])\n",
    "    return y"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "378e021c",
   "metadata": {},
   "source": [
    "### FX Functions\n",
    "We made a lot of different FX that we use in the processing of the individual tracks. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf1bfe03",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"FX processing functions\"\"\"\n",
    "\n",
    "def compressor(audio, threshold, makeup, smoothness):\n",
    "    x = np.array([])\n",
    "    x = np.append(x, audio)\n",
    "    for i in range(smoothness):\n",
    "        index = 0        \n",
    "        for i in x:        \n",
    "            if np.abs(i) >= threshold + 0.3:\n",
    "                x[index] = i * 0.85                 \n",
    "            elif np.abs(i) >= threshold + 0.2:\n",
    "                x[index] = i * 0.90            \n",
    "            elif np.abs(i) >= threshold + 0.1:\n",
    "                x[index] = i * 0.95            \n",
    "            elif np.abs(i) >= threshold:\n",
    "                x[index] = i * 0.99            \n",
    "            else:\n",
    "                x[index] = i\n",
    "            index += 1\n",
    "            \n",
    "    y = x * (1+makeup)\n",
    "    \n",
    "    return y\n",
    "\n",
    "def IRDelay(audio, impulses, sr=sr):\n",
    "    s = np.pad(audio, [0, audio.size])\n",
    "    s = fade(s, 1024)\n",
    "    \n",
    "    impulse_array = np.linspace(0.2, 0, num=impulses)\n",
    "    IRarray = np.array([])\n",
    "    size = int(s.size/impulses)-1\n",
    "    for i in range(impulses):\n",
    "\n",
    "        IRarray = np.append(IRarray, np.zeros(size))\n",
    "        IRarray = np.append(IRarray, impulse_array[i])\n",
    "\n",
    "    IRarray = sizecheck(IRarray, s)\n",
    "\n",
    "# transform to frequency domain\n",
    "    Y = scipy.fft.fft(s)\n",
    "    H = scipy.fft.fft(IRarray)\n",
    "\n",
    "# Doing the convolution in the frequency domain \n",
    "    output = Y*H\n",
    "\n",
    "# back to time domain\n",
    "    output = np.real(scipy.fft.ifft(output))\n",
    "    return output\n",
    "\n",
    "\n",
    "def allpass(delay, gain):\n",
    "    b = np.zeros(delay)\n",
    "    b[0] = gain\n",
    "    b[delay-1] = 1\n",
    "    a = np.zeros(delay)\n",
    "    a[0] = 1\n",
    "    a[delay-1] = gain\n",
    "    return b,a\n",
    "\n",
    "\n",
    "def IIRReverb(audio, combq, delay1, gain1, delay2, gain2):\n",
    "    \n",
    "    \"\"\"A Schroeder reverb with four parallell comb filters and two cascading allpass filters.\n",
    "    audio = the audio that you will apply the reverb on\n",
    "    combq = Q factor of the comb filters. Play around but 20 is a good place to start\n",
    "    delay1 = the delay of the first allpass filter, try settings around 1000\n",
    "    gain1 = the gain of the first allpass, try anything between 0 and 1, closer to 1 \n",
    "    results in more volume in the reverb\n",
    "    delay2 = same as delay1 but for the second allpass filter. Try a different and slightly longer\n",
    "    value than delay1, for example 3500\n",
    "    gain2 = same as gain1 but for the second allpass filter\"\"\"\n",
    "    \n",
    "    s_tail = audio\n",
    "    s_tail = np.append(s_tail, np.zeros(audio.size))    \n",
    "    \n",
    "    # First comb filter\n",
    "    f0 = 200  \n",
    "    Q = combq  \n",
    "    b1,a1 = signal.iircomb(f0, Q, ftype='notch', fs=sr)\n",
    "    y1 = signal.lfilter(b1,a1,s_tail)\n",
    "    # Second comb filter \n",
    "    f0 = 300  \n",
    "    Q = combq  \n",
    "    b2,a2 = signal.iircomb(f0, Q, ftype='notch', fs=sr)\n",
    "    y2 = signal.lfilter(b2,a2,s_tail)\n",
    "    # Third comb filter\n",
    "    f0 = 400  \n",
    "    Q = combq  \n",
    "    b3,a3 = signal.iircomb(f0, Q, ftype='notch', fs=sr)\n",
    "    y3 = signal.lfilter(b3,a3,s_tail)\n",
    "    # Fourth comb filter\n",
    "    f0 = 500 \n",
    "    Q = combq  \n",
    "    b4,a4 = signal.iircomb(f0, Q, ftype='notch', fs=sr)\n",
    "    y4 = signal.lfilter(b4,a4,s_tail)\n",
    "    # computing up the parallell structure by adding them together (and scaling the amplitude)\n",
    "    comb_result = (y1 + y2 + y3 + y4)*0.2\n",
    "    # Designing the two allpass filters\n",
    "    b1,a1 = allpass(delay1, gain1)\n",
    "    b2,a2 = allpass(delay2, gain2)\n",
    "\n",
    "    # Convolving the coefficients to get the cascading structure\n",
    "    cascade_a = np.convolve(a1,a2)\n",
    "    cascade_b = np.convolve(b1,b2)\n",
    "\n",
    "    # Run the result from the comb parallell filters through the cascade of allpass filters\n",
    "    signal_output = signal.lfilter(cascade_b, cascade_a, comb_result)\n",
    "    # Merging wet with dry\n",
    "    #audio = np.append(audio, np.zeros([signal_output.size-audio.size]))\n",
    "    #output = ((signal_output * wet)+audio)*0.5\n",
    "    \n",
    "    \n",
    "    return signal_output\n",
    "\n",
    "\n",
    "def framecompressor(audio, frame_length, threshold, ratio, makeup):\n",
    "    \n",
    "    \"\"\"audio = audio to be compressed\n",
    "    frame_length = compression window, try 2048\n",
    "    threshold = what value of amplitude will cause the compressor to compress, try 0.3\n",
    "    ratio = a float from 0 to 1, 0 will result in total gainremoval in the middle\n",
    "    of the compressor window and 1 will result in no compression. \n",
    "    makeup = how much you multiply the audio with after compression. Try 1.1 or 1.2\n",
    "    \n",
    "    \"\"\"\n",
    "    window = signal.get_window('hann', frame_length)    \n",
    "    b = np.array([-ratio])\n",
    "    c = signal.convolve(window, b)\n",
    "    window = c + 1\n",
    "    \n",
    "    x = np.array([])\n",
    "    x = np.append(x, audio)\n",
    "    \n",
    "    for i in range(0, len(audio), int(frame_length*0.5)):\n",
    "                \n",
    "        s = x[i:i + frame_length]        \n",
    "        a = np.max(np.abs(s))\n",
    "        if a > threshold:            \n",
    "            s = s * window\n",
    "            x[i:i+frame_length] = s\n",
    "        \n",
    "    compressed = x * makeup\n",
    "    return compressed\n",
    "\n",
    "def reverse(audio):\n",
    "    s = np.flip(audio)\n",
    "    return s\n",
    "\n",
    "\n",
    "def softClipper(audio, drive, output=0.8):\n",
    "\n",
    "    \"\"\"audio = Source to be SoftClipped\n",
    "    drive = Amount of SoftClipping (Try between 10-40)\n",
    "    output = Output volume\n",
    "    \n",
    "    The signal is normalized before output attenuation for better control.\n",
    "    \"\"\"\n",
    "    \n",
    "    # Drive can not be set to 0\n",
    "    if drive == 0:\n",
    "        drive = 1\n",
    "    \n",
    "    piDivisor = 2 / np.pi\n",
    "    driver = np.arctan(audio * drive)\n",
    "    \n",
    "    softclip = piDivisor * driver\n",
    "    \n",
    "    #normalized = softclip/np.max(softclip)\n",
    "    softClipped = softclip * output\n",
    "    \n",
    "    return softClipped\n",
    "\n",
    "def FIR_fft(audio, order, passband_hz, stopband_hz, pass_zero='lowpass', sr=sr):\n",
    "    \n",
    "    numtaps = order-1 # filter length (or number of tabs)\n",
    "    \n",
    "    wp = passband_hz/(sr/2) # passband edge normalized frequency\n",
    "    ws = stopband_hz/(sr/2) # stopband edge normalized frequency\n",
    "\n",
    "    freqs = [0.0, wp, ws, 1.0]\n",
    "    \n",
    "    if pass_zero == 'lowpass':\n",
    "        gains = [1.0, 1.0, 0.0, 0.0] # normalized gain values (0.5 = -3 dB)\n",
    "    elif pass_zero == 'highpass':\n",
    "        gains = [0.0, 0.0, 1.0, 1.0] # normalized gain values (0.5 = -3 dB)\n",
    "                \n",
    "    # Compute the FIR filter coefficients \n",
    "    h = signal.firwin2(numtaps, freqs, gains, window='hann') # compute FIR coefficients using firwin2 function\n",
    "    \n",
    "    # Design the FIR filter in the frequency domain \n",
    "    h_padded = np.pad(h,[0,len(audio)-len(h)]) # pad FIR coefficients with zeros in order to match the signal length\n",
    "    \n",
    "    # Apply the discrete Fourier Transform to the FIR filter (move to the frequency domain)\n",
    "    X = scipy.fft.fft(audio)\n",
    "    H = scipy.fft.fft(h_padded)\n",
    "    \n",
    "    Y = X*H # do the product of the DFT of the signal with the padded filter impulse response\n",
    "    \n",
    "    # Apply the inverse discrete Fourier Transform to the FIR filter (back to the time domain)\n",
    "    y = np.real(scipy.fft.ifft(Y))\n",
    "        \n",
    "    return y #signal filtered in the frequency domain"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32b9ff64",
   "metadata": {},
   "source": [
    "### Functions to resynthesize\n",
    "We made some different signal creating functions that could be used to resynthesize based on the midi information. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc796e1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def sawtooth_synth(freq, dur_ms, sr=sr, amp=1, phase=1):\n",
    "    t = np.arange(0, dur_ms/1000, 1/sr)\n",
    "    s = amp * signal.sawtooth(2 *np.pi*freq*t*phase)\n",
    "\n",
    "    return s\n",
    "\n",
    "def triangle_synth(freq, dur_ms, sr=sr, amp=1, phase=1, width=0.5):\n",
    "    t = np.arange(0, dur_ms/1000, 1/sr)\n",
    "    s = amp * signal.sawtooth(2 *np.pi*freq*t*phase)\n",
    "\n",
    "    return s\n",
    "\n",
    "def sine_synth(freq, dur_ms, sr=sr, amp=1, phase=1):\n",
    "    t = np.arange(0, dur_ms/1000, 1/sr)\n",
    "    s = amp*np.sin(2*np.pi*freq*t*phase)\n",
    "    \n",
    "    return s\n",
    "    \n",
    "\n",
    "def square_synth(freq, dur_ms, sr=sr, amp=1, phase=1):\n",
    "    t = np.arange(0, dur_ms/1000, 1/sr)\n",
    "    s = amp*signal.square(2*np.pi*freq*t*phase)\n",
    "    \n",
    "    return s\n",
    "\n",
    "\n",
    "# Repurposing the sine_synth as a LFO (for PWM).\n",
    "def myLFO(lfo_hz, dur_ms, lfo_depth):\n",
    "    if lfo_depth < 0.01:\n",
    "        lfo_depth = 0.01\n",
    "    elif lfo_depth > 0.99:\n",
    "        lfo_depth = 0.99\n",
    "    lfo = sine_synth(lfo_hz, dur_ms, amp=lfo_depth)\n",
    "    \n",
    "    return lfo\n",
    "\n",
    "\n",
    "def myPWM(freq, lfo_hz, dur_ms, lfo_depth=0.9):\n",
    "    t = np.linspace(0, 1, sr*(int(dur_ms/1000)), endpoint=False)\n",
    "    lfo = myLFO(lfo_hz, dur_ms, lfo_depth)\n",
    "    pwm = signal.square(2 * np.pi * freq * t, duty=(lfo + 1)/4)\n",
    "    \n",
    "    return pwm\n",
    "\n",
    "\n",
    "def chirp_sound(freq_start, freq_end, dur_ms, amp=1):\n",
    "    \n",
    "    t = np.arange(0, int((dur_ms/1000)*sr)) / sr\n",
    "    s = amp * signal.chirp(t, f0=freq_start, f1=freq_end, t1=dur_ms/1000, method='linear')\n",
    "\n",
    "    return s\n",
    "\n",
    "# Multiply the amp_envelope with your generated signal note by note \n",
    "# to get a nice smooth attack and decay\n",
    "\n",
    "def amp_envelope(dur_ms, sr=sr):\n",
    "    \n",
    "    t = np.arange(0, dur_ms/1000, 1/sr)\n",
    "    \n",
    "    # Splitting t up into 4 parts\n",
    "    attack_length = int(len(t) / 20)\n",
    "    decay_length = int(len(t) / 4)\n",
    "    sustain_length = int(len(t) / 2)\n",
    "    release_length = int(len(t) - (attack_length + decay_length + sustain_length))\n",
    "    \n",
    "    # Setting the start and stop amplitube values for the ramps\n",
    "    attack = np.linspace(0, 0.9, num=attack_length)\n",
    "    decay = np.linspace(0.9, 0.75, num=decay_length)\n",
    "    sustain = np.linspace(0.75, 0.6, num=sustain_length)\n",
    "    release = np.linspace(0.6, 0, num=release_length)\n",
    "    \n",
    "    # Putting them together in order\n",
    "    env = np.concatenate((attack, decay, sustain, release))\n",
    "    \n",
    "    return env"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22128ef3",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f74175b",
   "metadata": {},
   "source": [
    "### Midi\n",
    "Loading Midi and extracting information from the midi file. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cf1dd84",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#loading midi and displaying info\n",
    "\n",
    "pm = pretty_midi.PrettyMIDI('audio/py5.midi')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1bb0770",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a list based on the downbeats\n",
    "downbeats_in_seconds = pm.get_downbeats()\n",
    "\n",
    "# Converting it to sample positions\n",
    "downbeats_sample_time = downbeats_in_seconds*48000\n",
    "downbeats_sample_time = downbeats_sample_time. astype(int)\n",
    "downbeats_sample_time = downbeats_sample_time[1:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4a5d3af",
   "metadata": {},
   "source": [
    "# Mixing each track\n",
    "Here is each track going through it's own mixing/processing process. In the end of each track's cell we will display the waveplot of the original and the new track. There's also a small audio widget for listening to the mixed track. Each main block of code takes time to run, so have patience. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82afd175",
   "metadata": {},
   "source": [
    "### Hi hat track mixing\n",
    "Mixing the first drum track with hi hats and toms. Alternating between reverb, delay and reversed audio to create some movement.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e6296ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Have patience, it takes some time\n",
    "\n",
    "# Creating the result array we will add all signals back into\n",
    "result = np.zeros(s.size) \n",
    "\n",
    "# Temporary arrays used in the for loop\n",
    "segment = np.array([])\n",
    "seg = np.array([])\n",
    "\n",
    "# Setting parameters for the loop\n",
    "counter = 0\n",
    "current_beat = 0\n",
    "overlap = 0\n",
    "overlap_length = 1024\n",
    "\n",
    "\n",
    "for i in downbeats_sample_time:\n",
    "    # To check computational time and where it struggles, uncomment print(i). \n",
    "    #print(i)\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    # Slicing the segment from current_beat to the sample position i\n",
    "    seg = s[current_beat:i]    \n",
    "    # adding fade ins and outs to the segment\n",
    "    segment = fade(seg, overlap_length)\n",
    "    \n",
    "    # Processing segments one by one with different FX and signal processing\n",
    "    if counter == 1:\n",
    "        \n",
    "        segment = compressor(segment, 0.3, 1, 1)\n",
    "        seg_fx = IRDelay(segment, 8)\n",
    "        segment_padded = padder(segment, s, current_beat)\n",
    "        segment_fx = padder(seg_fx, s, current_beat)\n",
    "          \n",
    "        segment_padded = (segment_padded + segment_fx)\n",
    "                \n",
    "    elif counter == 2:\n",
    "        segment = compressor(segment, 0.3, 1, 1)\n",
    "        segment = softClipper(segment, 7)\n",
    "        segment_padded = padder(segment, s, current_beat)\n",
    "        \n",
    "    elif counter == 3:\n",
    "        segment = compressor(segment, 0.3, 1, 1)\n",
    "        seg_fx = IIRReverb(segment, 20, 800, 0.78, 1700, 0.68)\n",
    "        segment_padded = padder(segment, s, current_beat) \n",
    "        segment_fx = padder(seg_fx, s, current_beat)\n",
    "          \n",
    "        segment_padded = (segment_padded + segment_fx)\n",
    "\n",
    "    elif counter == 4: \n",
    "\n",
    "        segment = compressor(segment, 0.3, 1, 1)\n",
    "        segment = reverse(segment)\n",
    "        segment_padded = padder(segment, s, current_beat)\n",
    "    \n",
    "    # updating the current beat for the next loop\n",
    "    current_beat = current_beat + seg.size    \n",
    "    segment_overlapped = segment_padded[overlap:segment_padded.size]\n",
    "    \n",
    "    # getting the size right again by adding zeros to the end:\n",
    "    segment_final = np.pad(segment_overlapped, [0, overlap])\n",
    "\n",
    "    # Increasing overlap for next round of for loop \n",
    "    overlap = overlap + overlap_length\n",
    "    \n",
    "    # Just making sure the sizes are allright \n",
    "    segment_final = sizecheck(segment_final, result)\n",
    "    \n",
    "    # Resetting the counter when it reaches 4\n",
    "    if counter == 4:\n",
    "        counter = 0\n",
    "    # pasting the segments back in, one by one\n",
    "    result = result + segment_final\n",
    "\n",
    "\n",
    "# Plotting the original track compared to the result after mixing it\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveplot(s, sr=sr)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveplot(result, sr=sr)\n",
    "plt.show()\n",
    "hihats = result\n",
    "\n",
    "ipd.display(ipd.Audio(s, rate=sr))\n",
    "ipd.display(ipd.Audio(hihats, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5893286a",
   "metadata": {},
   "source": [
    "### Main drums mixing\n",
    "Mixing the kick and snare drum track. Applying a moving low pass filter in the beginning, then opening it up. The drums are compressed as well. Adding a low downward frequency sweep for flavour on the third beat of every measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10d3042c",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Creating the result array we will add all signals back into\n",
    "result = np.zeros(s.size) \n",
    "\n",
    "# Temporary arrays used in the for loop\n",
    "segment = np.array([])\n",
    "seg = np.array([])\n",
    "\n",
    "# Setting parameters for the loop\n",
    "counter = 0\n",
    "current_beat = 0\n",
    "overlap = 0\n",
    "overlap_length = 1024\n",
    "\n",
    "passband = 100\n",
    "stopband = 150\n",
    "\n",
    "chirpseg = np.zeros(s2.size)\n",
    "chirpsound = chirp_sound(65, 261, 850, amp=0.1)\n",
    "\n",
    "for i in downbeats_sample_time:\n",
    "    # Uncomment to check computational time\n",
    "    #print(i)\n",
    "    counter += 1\n",
    "    \n",
    "    # Slicing the segment from current_beat to the sample position i\n",
    "    seg = s2[current_beat:i]    \n",
    "    # adding fade ins and outs to the segment\n",
    "    segment = fade(seg, overlap_length)\n",
    "    # Adding some synthesized audio to the beat   \n",
    "    if np.max(seg) > 0.2:         \n",
    "        chirpsound = fade(chirpsound, overlap_length)            \n",
    "        chirpseg[current_beat+57090:current_beat+57090+chirpsound.size] = chirpsound    \n",
    "    \n",
    "\n",
    "    # Processing with compressor\n",
    "    segment = framecompressor(segment, 2048, 0.6, 0.75, 1.2)\n",
    "    if counter < 10:\n",
    "        \n",
    "        segment = FIR_fft(segment, 9, passband, stopband, pass_zero='lowpass', sr=sr) \n",
    "        \n",
    "        passband += 100\n",
    "        stopband += 100\n",
    "            \n",
    "    segment_padded = padder(segment, s2, current_beat)\n",
    "    \n",
    "    segment_padded = segment_padded + (chirpseg*0.2)\n",
    "    \n",
    "    # updating the current beat for the next loop\n",
    "    current_beat = current_beat + seg.size    \n",
    "    segment_overlapped = segment_padded[overlap:segment_padded.size]\n",
    "    \n",
    "    # getting the size right again by adding zeros to the end:\n",
    "    segment_final = np.pad(segment_overlapped, [0, overlap])\n",
    "\n",
    "    # Increasing overlap for next round of for loop \n",
    "    overlap = overlap + overlap_length\n",
    "    \n",
    "    # Just making sure the sizes are allright \n",
    "    segment_final = sizecheck(segment_final, result)\n",
    "    \n",
    "    \n",
    "    # pasting the segments back in, one by one\n",
    "    result = result + segment_final\n",
    "\n",
    "# Plotting the original track compared to the result after mixing it\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveplot(s2, sr=sr)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveplot(result, sr=sr)\n",
    "plt.show() \n",
    "drums = result\n",
    "ipd.display(ipd.Audio(s2, rate=sr))\n",
    "ipd.display(ipd.Audio(drums, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72f08066",
   "metadata": {},
   "source": [
    "### Pluck Synth mixing\n",
    "Mixing the pluck synth track. Adding reverb and saturation, and a moving low pass filter.\n",
    "Delay in the end to make it more interesting. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4082ac92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating the result array we will add all signals back into\n",
    "result = np.zeros(s.size) \n",
    "\n",
    "# Temporary arrays used in the for loop\n",
    "segment = np.array([])\n",
    "seg = np.array([])\n",
    "\n",
    "# Setting parameters for the loop\n",
    "counter = 0\n",
    "current_beat = 0\n",
    "overlap = 0\n",
    "overlap_length = 1024\n",
    "stopband = 550\n",
    "passband = 450\n",
    "\n",
    "for i in downbeats_sample_time:\n",
    "    # To check computational time and where it struggles, uncomment print(i). \n",
    "    # print(i)\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    # Slicing the segment from current_beat to the sample position i\n",
    "    seg = s3[current_beat:i]    \n",
    "    # adding fade ins and outs to the segment\n",
    "    segment = fade(seg, overlap_length)\n",
    "    \n",
    "    # Processing segments one by one with different FX and signal processing\n",
    "    if counter < 5:\n",
    "        # silencing it\n",
    "        segment = segment * 0\n",
    "        segment_padded = padder(segment, s3, current_beat)\n",
    "                                  \n",
    "    elif counter >= 18:\n",
    "        segment = IIRReverb(segment, 20, 1000, 0.78, 4200, 0.68)\n",
    "        segment = softClipper(segment, 23)\n",
    "        segmentFX = IRDelay(segment, 8)\n",
    "        segment_padded = padder(segment, s3, current_beat)\n",
    "        segmentFX_padded = padder(segmentFX, s3, current_beat)\n",
    "        segment_padded = (segment_padded + segmentFX_padded) * 0.5\n",
    "\n",
    "    elif counter >= 5:\n",
    "        segment = IIRReverb(segment, 20, 1000, 0.78, 4200, 0.68)\n",
    "        segment = softClipper(segment, 23)\n",
    "        segment = FIR_fft(segment, 21, passband, stopband, pass_zero='lowpass', sr=sr) \n",
    "        stopband += 100\n",
    "        passband += 100\n",
    "        #segment = segment * 0.5        \n",
    "        segment_padded = padder(segment, s3, current_beat)\n",
    "    \n",
    "    # updating the current beat for the next loop\n",
    "    current_beat = current_beat + seg.size    \n",
    "    segment_overlapped = segment_padded[overlap:segment_padded.size]\n",
    "    \n",
    "    # getting the size right again by adding zeros to the end:\n",
    "    segment_final = np.pad(segment_overlapped, [0, overlap])\n",
    "\n",
    "    # Increasing overlap for next round of for loop \n",
    "    overlap = overlap + overlap_length\n",
    "    \n",
    "    # Just making sure the sizes are allright \n",
    "    segment_final = sizecheck(segment_final, result)\n",
    "    \n",
    "    \n",
    "    # pasting the segments back in, one by one\n",
    "    result = result + segment_final\n",
    "\n",
    "# Plotting the original track compared to the result after mixing it\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveplot(s3, sr=sr)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveplot(result, sr=sr)\n",
    "plt.show()\n",
    "pluck_synth = result\n",
    "ipd.display(ipd.Audio(s3, rate=sr))\n",
    "ipd.display(ipd.Audio(pluck_synth, rate=sr))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "561082f2",
   "metadata": {},
   "source": [
    "# Resynthesizing the Pluck synth part \n",
    "The pluck synth needed some magic, so it is resynthesized here. \n",
    "We use it to have two different pluck synths playing in left and right in the final mix. \n",
    "Time did not enable us to actually resynthesize it with any of our signal creating functions but we will certainly try that for the portifolio. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3c2832a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Midi again\n",
    "\n",
    "# Extracting the right instrument\n",
    "plucksynth = pm.instruments[2]\n",
    "# Resynthesizing the midi using pretty_midis function\n",
    "resynthed = plucksynth.synthesize(fs=48000, wave=signal.square)\n",
    "# Applying saturation, reverb and filter\n",
    "resynthed = softClipper(resynthed, 23)\n",
    "resynthed = IIRReverb(resynthed, 20, 1000, 0.78, 4200, 0.68)\n",
    "resynthed = FIR_fft(resynthed, 21, 800, 1300, pass_zero='lowpass', sr=sr)\n",
    "\n",
    "resynthed_plucks = resynthed[0:s3.size]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c4d3565",
   "metadata": {},
   "source": [
    "# Mixing the brass stabs \n",
    "Adding filtering, a moving delay and even reversing the track every fourth measure. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508748c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BRASS STABS MIXING\n",
    "\n",
    "# Creating the result array where all signals will be added into\n",
    "result = np.zeros(brass.size) \n",
    "\n",
    "# Temporary arrays used in the for loop\n",
    "segment = np.array([])\n",
    "seg = np.array([])\n",
    "\n",
    "# Making the counter\n",
    "counter = 0\n",
    "current_beat = 0\n",
    "overlap = 0\n",
    "overlap_length = 1024\n",
    "\n",
    "# Arguments' initial values\n",
    " # Frame compressor\n",
    "frame_length = 2048\n",
    "threshold = 0.3\n",
    "ratio = 0.4\n",
    "makeup = 1.5\n",
    " # FIR fft\n",
    "order = 21\n",
    "passband = 400\n",
    "stopband = 550\n",
    "pass_zero = 'lowpass'\n",
    " # IIRReverb\n",
    "combq = 40\n",
    "delay1 = 1100\n",
    "gain1 = 0.48\n",
    "delay2 = 1500\n",
    "gain2 = 0.38\n",
    " # IRDelay\n",
    "impulses = 7\n",
    "\n",
    "for i in downbeats_sample_time:\n",
    "    # Uncomment to check computational time and where it struggles\n",
    "    # print(i)\n",
    "    \n",
    "    counter += 1\n",
    "    \n",
    "    # Slicing the segment from current_beat to the sample position i\n",
    "    seg = brass[current_beat:i]    \n",
    "    # Adding fade ins and outs to the segment\n",
    "    segment = fade(seg, overlap_length)\n",
    "       \n",
    "    if counter == 1:\n",
    "       \n",
    "        segment = framecompressor(segment, frame_length, threshold, ratio, makeup)\n",
    "        seg_lop = FIR_fft(segment, order, passband, stopband, pass_zero=pass_zero, sr=sr) \n",
    "        segment_padded = np.pad(segment,[current_beat, brass.size-(current_beat+segment.size)]) \n",
    "        segment_lop = np.pad(seg_lop,[current_beat, brass.size-(current_beat+seg_lop.size)])\n",
    "\n",
    "          \n",
    "        segment_padded = (segment_padded + segment_lop)\n",
    "        \n",
    "        # Updated effects values\n",
    "        passband += 50\n",
    "        stopband += 50\n",
    "        \n",
    "    elif counter == 2:\n",
    "        \n",
    "        segment = reverse(segment)\n",
    "        seg_lop = FIR_fft(segment, order, passband, stopband, pass_zero=pass_zero, sr=sr) \n",
    "        seg_del = IRDelay(segment, impulses, sr=sr)\n",
    "        segment_padded = np.pad(segment,[current_beat, brass.size-(current_beat+segment.size)]) \n",
    "        segment_lop = np.pad(seg_lop,[current_beat, brass.size-(current_beat+seg_lop.size)])\n",
    "        segment_del = np.pad(seg_del,[current_beat, brass.size-(current_beat+seg_del.size)])\n",
    "        \n",
    "          \n",
    "        segment_padded = (segment_padded + segment_lop + segment_del)\n",
    "        \n",
    "        # Updated effects values\n",
    "        passband += 50\n",
    "        stopband += 50\n",
    "        \n",
    "    elif counter == 3:\n",
    "        \n",
    "        segment = framecompressor(segment, frame_length, threshold, ratio, makeup)\n",
    "        seg_lop = FIR_fft(segment, order, passband, stopband, pass_zero=pass_zero, sr=sr)\n",
    "        seg_del = IRDelay(segment, impulses+1, sr=sr)\n",
    "        segment_padded = np.pad(segment,[current_beat, brass.size-(current_beat+segment.size)]) \n",
    "        segment_lop = np.pad(seg_lop,[current_beat, brass.size-(current_beat+seg_lop.size)])\n",
    "        segment_del = np.pad(seg_del,[current_beat, brass.size-(current_beat+seg_del.size)])\n",
    "\n",
    "          \n",
    "        segment_padded = (segment_padded + segment_lop + segment_del)\n",
    "        \n",
    "       # Updated effects values\n",
    "        passband += 50\n",
    "        stopband += 50\n",
    "\n",
    "    elif counter == 4: \n",
    "        \n",
    "        seg_lop = FIR_fft(segment, order, passband, stopband, pass_zero=pass_zero, sr=sr)\n",
    "        seg_del = IRDelay(segment, impulses+1, sr=sr)\n",
    "        seg_rev = IIRReverb(segment, combq, delay1, gain1, delay2, gain2)\n",
    "        segment_padded = np.pad(segment,[current_beat, brass.size-(current_beat+segment.size)])\n",
    "        segment_lop = np.pad(seg_lop,[current_beat, brass.size-(current_beat+seg_lop.size)])\n",
    "        segment_del = np.pad(seg_del,[current_beat, brass.size-(current_beat+seg_del.size)])\n",
    "        segment_rev = np.pad(seg_rev,[current_beat, brass.size-(current_beat+seg_rev.size)])\n",
    "        \n",
    "\n",
    "        segment_padded = (segment_padded + segment_lop + segment_del + segment_rev)\n",
    "        \n",
    "        # Updated effects values\n",
    "        passband += 50\n",
    "        stopband += 50\n",
    "        gain1 = 0.10\n",
    "        gain2 += 0.05\n",
    "    \n",
    "    \n",
    "    # Updating the current beat for the next loop\n",
    "    current_beat = current_beat + seg.size\n",
    "\n",
    "    \n",
    "    segment_overlapped = segment_padded[overlap:segment_padded.size]\n",
    "    \n",
    "    # Getting the size right again by adding zeros to the end:\n",
    "    segment_final = np.pad(segment_overlapped, [0, overlap])\n",
    "    # Increasing overlap for next round of for loop \n",
    "    overlap = overlap + overlap_length\n",
    "    \n",
    "    # Just making sure the sizes are allright \n",
    "    segment_final = sizecheck(segment_final, result)\n",
    "    \n",
    "    # Resetting the counter when it reaches 4\n",
    "    if counter == 4:\n",
    "        counter = 0\n",
    "    # Pasting the segments back in, one by one\n",
    "    result = result + segment_final\n",
    "    \n",
    "# Playing back the result\n",
    "ipd.display(ipd.Audio(brass, rate=sr))\n",
    "ipd.display(ipd.Audio(result, rate=sr))\n",
    "\n",
    "# Plotting the original track compared to the result after mixing it\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveplot(brass, sr=sr)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveplot(result, sr=sr)\n",
    "plt.show()\n",
    "\n",
    "# Re-name the result to add to the other instruments later\n",
    "bras = result"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e2b0d18",
   "metadata": {},
   "source": [
    "# Bass synth mixing\n",
    "Filtering, compressing and distorting it. Lowpass filters opens up as the song progresses. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4946ece3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# BASS SYNTH MIXING\n",
    "\n",
    "# Creating the result array where all signals will be added into\n",
    "result = np.zeros(bass.size) \n",
    "\n",
    "# Temporary arrays used in the for loop\n",
    "segment = np.array([])\n",
    "seg = np.array([])\n",
    "\n",
    "# Setting variables for the loop\n",
    "current_beat = 0\n",
    "overlap = 0\n",
    "overlap_length = 1024\n",
    "\n",
    "# Arguments' initial values\n",
    " # Frame compressor\n",
    "frame_length = 2048\n",
    "threshold = 0.3\n",
    "ratio = 0.6\n",
    "makeup = 1.5\n",
    " # FIR fft\n",
    "order = 21\n",
    "passband = 60\n",
    "stopband = 90\n",
    "pass_zero = \"lowpass\"\n",
    " # IIRReverb\n",
    "combq = 40\n",
    "delay1 = 1100\n",
    "gain1 = 0.48\n",
    "delay2 = 1500\n",
    "gain2 = 0.38\n",
    " # softClipper\n",
    "drive = 30\n",
    "output = 0.9\n",
    "\n",
    "\n",
    "for i in downbeats_sample_time:\n",
    "    # Uncomment to check computational time and where it struggles:\n",
    "    # print(i)\n",
    "    \n",
    "    # Slicing the segment from current_beat to the sample position i\n",
    "    seg = bass[current_beat:i]    \n",
    "    # Adding fade ins and outs to the segment\n",
    "    segment = fade(seg, overlap_length)\n",
    "       \n",
    "    \n",
    "    segment = framecompressor(segment, frame_length, threshold, ratio, makeup)\n",
    "    segment = FIR_fft(segment, order, passband, stopband, pass_zero=pass_zero, sr=sr)\n",
    "    segment = softClipper(segment, drive, output)\n",
    "    segment_padded = padder(segment, bass, current_beat)\n",
    "                \n",
    "    # Updated effects values\n",
    "    passband += 51\n",
    "    stopband += 61\n",
    "            \n",
    "    # Updating the current beat for the next loop\n",
    "    current_beat = current_beat + seg.size\n",
    "    \n",
    "    segment_overlapped = segment_padded[overlap:segment_padded.size]\n",
    "    \n",
    "    # Getting the size right again by adding zeros to the end:\n",
    "    segment_final = np.pad(segment_overlapped, [0, overlap])\n",
    "    # Increasing overlap for next round of for loop \n",
    "    overlap = overlap + overlap_length\n",
    "    \n",
    "    # Just making sure the sizes are allright \n",
    "    segment_final = sizecheck(segment_final, result)\n",
    "        \n",
    "    # Pasting the segments back in, one by one\n",
    "    result = result + segment_final\n",
    "    \n",
    "# Playing back the result\n",
    "ipd.display(ipd.Audio(bass, rate=sr))\n",
    "ipd.display(ipd.Audio(result, rate=sr))\n",
    "result = result/np.max(result)\n",
    "bas = result\n",
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveplot(bass, sr=sr)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveplot(bas, sr=sr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13b52fe2",
   "metadata": {},
   "source": [
    "# Mixing it all together\n",
    "So the challenge here is to get a mix of everything together sounding half decent. It is aestethically speaking quite lo fi, and of course no sane person would attempt to do their mix in Python with home made signal processing. But we tried! And we had fun. \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b9a3aff",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Stereo Mix\n",
    "\n",
    "mixL = (hihats*0.5) + (drums*0.9) + (pluck_synth*0.7) + (bras*1) + (bas*0.7)\n",
    "mixR = (hihats*0.5) + (drums*0.9) + (pluck_synth*0.2) + (resynthed_plucks*0.3) + (bras*0.7) + (bas*0.7)\n",
    "\n",
    "# Normalizing the mix\n",
    "mixL = mixL * 0.75\n",
    "mixR = mixR * 0.75\n",
    "\n",
    "MIX = np.array([mixL, mixR])\n",
    "\n",
    "# Mono mix\n",
    "\n",
    "mix = (hihats*0.4) + (drums*0.8) + (pluck_synth*0.7) + (bras) + (bas*0.7) + (resynthed_plucks*0.3)\n",
    "mix = mix * 0.75\n",
    "\n",
    "ipd.Audio(MIX, rate=sr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aafe73c5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Writing the mix to disk\n",
    "MIX.T\n",
    "sf.write('audio/stereo_file.wav', MIX.T, sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7842130d",
   "metadata": {},
   "source": [
    "# Visually comparing the original to the new mix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194f132c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8, 4))\n",
    "plt.subplot(2,1,1)\n",
    "librosa.display.waveplot(org_mix, sr=sr)\n",
    "plt.subplot(2,1,2)\n",
    "librosa.display.waveplot(mix, sr=sr)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011baa54",
   "metadata": {},
   "source": [
    "## Visualizations\n",
    "\n",
    "#### Audacity (Peak + RMS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bbff27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import features.py to access our RMS-function\n",
    "import features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c54269ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "## A function to display in the style of Audacity\n",
    "def vizAudacity(audio):\n",
    "\n",
    "    frame_length = 512\n",
    "    \n",
    "    # Finding the RMS\n",
    "    rms_values = features.root_mean_square(audio, 2048, frame_length)\n",
    "    \n",
    "    # Making a waveform with the RMS values\n",
    "    rms_audio = []\n",
    "    count = 0\n",
    "    \n",
    "    for i in range(0, len(audio), frame_length): #in your case 512\n",
    "        chunk = audio[i:i + frame_length]\n",
    "        try:\n",
    "            a = rms_values[count]*chunk\n",
    "        except:\n",
    "            break\n",
    "        rms_audio = np.append(rms_audio, a)\n",
    "        count += 1\n",
    "    \n",
    "    # Choosing style and colors for the plot\n",
    "    plt.style.use('seaborn-white')\n",
    "    fig, ax = plt.subplots(figsize=(16, 4), facecolor='#e7e7e7', dpi=150.0)\n",
    "    ax.set_facecolor('#c0c0c0')\n",
    "\n",
    "    # Setting labels\n",
    "    ax.set_title(\"Audacity style waveform\", fontsize='24', weight='roman', y=1.02, family='monospace')\n",
    "    ax.set_xmargin(0.005)\n",
    "\n",
    "    # Plotting the outer waveform (Peak/Amplitude)\n",
    "    ax.plot(audio, color='#3132c1')\n",
    "\n",
    "    # Plotting the inner waveform (RMS)\n",
    "    ax.plot(rms_audio, color='#6464d5')\n",
    "\n",
    "    # Generating lists for showing x-axis ticks every 15 sec,\n",
    "    # by finding the floor value of (length // 15) + 1 for the np.arange().\n",
    "    tick_stop = ((len(s)/sr)//15)+1\n",
    "    s_ticks_value = np.arange(sr*15,sr*15*tick_stop,sr*15)\n",
    "    s_ticks = np.arange(int(15),int(15*tick_stop),15)\n",
    "\n",
    "    # Showing minutes instead of samples\n",
    "    ax.set_xticks(s_ticks_value)\n",
    "    ax.set_xticklabels(s_ticks)\n",
    "    ax.axis(ymin=-1.1, ymax=1.1)\n",
    "\n",
    "    # Displaying the plot\n",
    "    fig.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f643dd98",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "vizAudacity(mix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecaa49d2",
   "metadata": {},
   "source": [
    "#### Traktor/Serato (Colored by Spectral Centroid)\n",
    "```Colored Waveforms\n",
    "\n",
    "Colors in the waveform show the frequency of the sound:\n",
    "bass is red, mids are green, treble is blue.\n",
    "                                        (www.serato.com)\n",
    "```\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7286111",
   "metadata": {},
   "outputs": [],
   "source": [
    "def vizSerato(audio, sr):\n",
    "\n",
    "    # Locate note onset events by picking peaks in an onset strength envelope\n",
    "    onsets = librosa.onset.onset_detect(y=audio, sr=sr, units='samples', backtrack=True, \n",
    "                                        wait=1, pre_avg=1, post_avg=1, pre_max=5, post_max=5)\n",
    "\n",
    "    if len(onsets) > 80:\n",
    "        onsets = librosa.onset.onset_detect(y=audio, sr=sr, units='samples', backtrack=True, \n",
    "                                        wait=20, pre_avg=1, post_avg=1, pre_max=5, post_max=5)\n",
    "    \n",
    "    # Choosing style and colors for the plot\n",
    "    plt.style.use('seaborn-white')\n",
    "    fig, ax = plt.subplots(figsize=(16, 4), facecolor='#373737', dpi=150.0)\n",
    "    ax.set_facecolor('#000000')\n",
    "    \n",
    "    # Setting labels\n",
    "    ax.set_title(\"Serato/Traktor style waveform\", fontsize='24', weight='roman', y=1.02, family='monospace', color='#f4f4f4')\n",
    "    ax.set_xmargin(0.005)\n",
    "    \n",
    "    for i in range(len(onsets)-1):\n",
    "\n",
    "        segment = audio[onsets[i]:onsets[i+1]]\n",
    "\n",
    "        # Compute spectral centroid for each segment\n",
    "        centroid = features.spectral_centroid(segment, sr=sr, frame_length=len(segment), hop_length=len(segment)*2)\n",
    "        x = np.pad(segment, (onsets[i], 0), 'constant')\n",
    "\n",
    "        if centroid <= 59: # Sub Low\n",
    "            plt.plot(x, color='#113450')\n",
    "\n",
    "        elif 60 <= centroid <= 249: # Low\n",
    "            plt.plot(x, color='#4978c0')\n",
    "\n",
    "        elif 250 <= centroid <= 600: # Low Mid\n",
    "            plt.plot(x, color='#8a61d3')        \n",
    "\n",
    "        elif 601 <= centroid <= 900: # Mid\n",
    "            plt.plot(x, color='#199eb0')\n",
    "\n",
    "        elif 901 <= centroid <= 1500: # High Mid\n",
    "            plt.plot(x, color='#ffc0cb')\n",
    "\n",
    "        elif 1501 <= centroid: # High\n",
    "            plt.plot(x, color='#68e17f')\n",
    "            \n",
    "    # Generating lists for showing x-axis ticks every 15 sec,\n",
    "    # by finding the floor value of (length // 15) + 1 for the np.arange().\n",
    "    tick_stop = ((len(s)/sr)//5)+1\n",
    "    s_ticks_value = np.arange(sr*5,sr*5*tick_stop,sr*5)\n",
    "    s_ticks = np.arange(int(5),int(5*tick_stop),5)\n",
    "\n",
    "    # Showing minutes instead of samples\n",
    "    ax.set_xticks(s_ticks_value)\n",
    "    ax.set_xticklabels(s_ticks, color='#f4f4f4')\n",
    "    ax.tick_params(axis='y', colors='#f4f4f4')\n",
    "    ax.set_xlabel('Time (seconds)', color='#f4f4f4')\n",
    "    \n",
    "    plt.savefig('serato.png')\n",
    "    \n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "307390a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "vizSerato(mix, sr=sr)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c5603bc",
   "metadata": {},
   "source": [
    "# Final Reflections\n",
    "\n",
    "This program takes a LONG time to run and the results aren't optimal, but we kind of knew that before we started out. So efficiency and quality aside, we can definetely say that we have created a unique mix that it would be very hard to replicate without the code, so in terms of originality the audio result can be said to succesfull (not meant as a review of the music itself, but the sound). "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
